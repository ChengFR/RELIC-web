<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=700" />
    <meta
      name="RELIC"
      content="Demo page for the RELIC paper."
    />

    <link href="https://fonts.googleapis.com/css2?family=Poppins" rel="stylesheet">
    <title>RELIC</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>

    <div id="content_column">
    <h3 style="font-size: 33pt;">
      RELIC: Investigating Large Language Model Responses using Self-Consistency
    </h3>

    <div id="authors">
      <a href="https://www.furuicheng.tech/">Furui Cheng</a>,
      <a href="https://vilda.net/">Vilém Zouhar</a>,
      <a href="https://arorasimran.com/">Simran Arora</a>,
      <a href="https://www.mrinmaya.io/">Mrinmaya Sachan</a>,
      <a href="http://hendrik.strobelt.com/">Hendrik Strobelt</a>,
      <a href="https://el-assady.com/">Mennatallah El-Assady</a>
    </div>

    <p style="font-size: medium; text-align: justify;">
      Large Language Models (LLMs) are notorious for blending fact with fiction and generating non-factual content, known as hallucinations. To tackle this challenge, we propose an interactive system that helps users obtain insights into the reliability of the generated text. Our approach is based on the idea that the self-consistency of multiple samples generated by the same LLM relates to its confidence in individual claims in the generated texts. Using this idea, we design RELIC, an interactive system that enables users to investigate and verify semantic-level variations in multiple long-form responses. This allows users to recognize potentially inaccurate information in the generated text and make necessary corrections. From a user study with ten participants, we demonstrate that our approach helps users better verify the reliability of the generated text. We further summarize the design implications and lessons learned from this research for inspiring future studies on reliable human-LLM interactions. 
    </p>
    
    <a href="https://arxiv.org/abs/2311.16842" class="link_button" style="background-color: #b11;">Preprint</a>
    <a href="https://github.com/ChengFR/RELIC-web/" class="link_button" style="background-color: #118;">Code (WIP)</a>
    <a href="https://github.com/ChengFR/RELIC-web/" class="link_button" style="background-color: #118;">Demo (WIP)</a>

    <br><br>

    <iframe
      width="560" height="315"
      src="https://www.youtube.com/embed/V7kfbnuBqFI?si=X5vne0XAxSlyoa5G"
      title="short presentation of RELIC" frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen
      style="margin-left: auto; margin-right: auto; display: block; margin-bottom: 10px;"
    ></iframe>

    <div id="bibtex_popup" style="display: inline-block;"
      >@misc{cheng2023relic,<br>
        &nbsp;&nbsp;title={RELIC: Investigating Large Language Model Responses using Self-Consistency},<br>
        &nbsp;&nbsp;author={Furui Cheng and Vilém Zouhar and Simran Arora and Mrinmaya Sachan and Hendrik Strobelt and Mennatallah El-Assady},<br>
        &nbsp;&nbsp;year={2023},<br>
        &nbsp;&nbsp;eprint={2311.16842},<br>
        &nbsp;&nbsp;archivePrefix={arXiv},<br>
        &nbsp;&nbsp;primaryClass={cs.HC}<br>
      }
    </div>
  </div>

  <div id="root"></div>
  </body>
</html>
